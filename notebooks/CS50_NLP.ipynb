{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b28f53-f926-405f-8d00-98027e2a9aa1",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) with Dimensions Grant Data\n",
    "\n",
    "This notebook introduces Natural Language Processing (NLP) concepts using **Dimensions-style grant abstracts** as real-world examples.\n",
    "\n",
    "1. **Fundamentals**\n",
    "   - Tokenization\n",
    "   - n-grams\n",
    "   - Bag-of-Words\n",
    "   - Markov text generation\n",
    "\n",
    "2. **Classification (Naive Bayes)**\n",
    "   - Classify grants as AI vs non-AI using text\n",
    "\n",
    "3. **Word Representations**\n",
    "   - One-hot vectors\n",
    "   - Word embeddings (Word2Vec, Skip-Gram)\n",
    "\n",
    "4. **Syntax & Semantics**\n",
    "   - Context-Free Grammar parsing with NLTK\n",
    "   - Syntactic ambiguity & semantics\n",
    "\n",
    "5. **Neural NLP**\n",
    "   - RNN text classifier\n",
    "   - Attention concepts\n",
    "   - Transformer sentence embeddings\n",
    "\n",
    "6. **Applications in Grants Data**\n",
    "   - Summarization\n",
    "   - Extraction\n",
    "   - Language ID\n",
    "   - Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197f9120-7979-432a-a6d2-024d706a41ea",
   "metadata": {},
   "source": [
    "# Imports & Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b085e9c-7c00-490c-9ff8-e6b917742d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP tools\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk import CFG, ChartParser\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Word embeddings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Deep NLP\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, preprocessing\n",
    "\n",
    "# Ensure NLTK components\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# --- Load Dimensions grant data (must include \"abstract\" and \"is_ai_ml\") ---\n",
    "# grants = pd.read_csv(\"grants.csv\")\n",
    "\n",
    "print(\"Columns:\", grants.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b91099-7067-48e6-9090-f71c5feeec3f",
   "metadata": {},
   "source": [
    "## 1. Tokenization & N-grams\n",
    "\n",
    "Tokenization splits text into words and sentences.  \n",
    "n-grams capture short sequences (e.g., bigrams, trigrams) useful for prediction and topic exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a688a-a503-4058-8e8c-9e8539412120",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = grants[\"abstract\"].dropna().iloc[0]\n",
    "print(\"Sample grant abstract:\\n\", sample_text)\n",
    "\n",
    "# Word tokenization\n",
    "tokens = word_tokenize(sample_text.lower())\n",
    "print(\"\\nTokens:\", tokens[:20])\n",
    "\n",
    "# Bigrams\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(\"\\nExample bigrams:\", bigrams[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e445e16b-78cb-41fe-89cf-d2e0186a05f4",
   "metadata": {},
   "source": [
    "## 2. Bag-of-Words Representation\n",
    "\n",
    "Bag-of-Words ignores order and simply counts word occurrences.  \n",
    "Useful for:\n",
    "- Topic features\n",
    "- Text classification\n",
    "- Similarity queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984b47a-4d24-4342-9be5-b433878b0bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=2000, stop_words=\"english\")\n",
    "X_bow = vectorizer.fit_transform(grants[\"abstract\"].fillna(\"\"))\n",
    "\n",
    "print(\"Shape (documents × vocab):\", X_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173b0f9-e1a5-4d28-ad47-e3c84a3f163c",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes Text Classifier (AI vs Non-AI)\n",
    "\n",
    "Naive Bayes assumes word independence (with smoothing).  \n",
    "We’ll classify grants into two groups based purely on their abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a95299-dddf-4805-9737-0e96e6f5c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = grants[\"is_ai_ml\"].astype(int).fillna(0).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_bow, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "pred = nb.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a6774-af0a-4685-af18-f76468829500",
   "metadata": {},
   "source": [
    "## 4. Markov Model (n-gram text generator)\n",
    "\n",
    "A Markov chain predicts the next word based on previous words.  \n",
    "We build a simple *bigram* model from Dimensions abstracts to generate synthetic grant descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c2417-ecbb-4235-84b9-579f3a2a36a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Build transitions: P(next | current)\n",
    "transitions = defaultdict(list)\n",
    "\n",
    "for abstract in grants[\"abstract\"].dropna().head(500):\n",
    "    words = [\"<s>\"] + word_tokenize(abstract.lower()) + [\"</s>\"]\n",
    "    for w1, w2 in ngrams(words, 2):\n",
    "        transitions[w1].append(w2)\n",
    "\n",
    "def generate_text(seed=\"<s>\", length=25):\n",
    "    word = seed\n",
    "    result = []\n",
    "    for _ in range(length):\n",
    "        next_words = transitions.get(word, [\"</s>\"])\n",
    "        word = random.choice(next_words)\n",
    "        if word == \"</s>\":\n",
    "            break\n",
    "        result.append(word)\n",
    "    return \" \".join(result)\n",
    "\n",
    "print(generate_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad4add-74c1-49ac-b3ee-5f461d482184",
   "metadata": {},
   "source": [
    "## 5. Word Representations\n",
    "\n",
    "### One-hot vectors\n",
    "- Simple but high-dimensional\n",
    "- No notion of similarity\n",
    "\n",
    "### Embeddings (Word2Vec)\n",
    "- Dense vectors capturing semantic similarity\n",
    "- Use Skip-Gram to learn meaning from context\n",
    "- Example: king – man + woman ≈ queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9122b4a-6d12-4a4d-a694-2e3491033fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_tokens = [word_tokenize(a.lower()) for a in grants[\"abstract\"].dropna().head(5000)]\n",
    "w2v = Word2Vec(abstract_tokens, vector_size=50, window=5, min_count=3, workers=4)\n",
    "\n",
    "print(\"Most similar to 'data':\", w2v.wv.most_similar(\"data\")[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae6be8-a5a0-4d96-883a-2a924e6027a7",
   "metadata": {},
   "source": [
    "## 6. Syntax & Semantics with a CFG\n",
    "\n",
    "Syntax = structure  \n",
    "Semantics = meaning\n",
    "\n",
    "We demonstrate a **Context-Free Grammar (CFG)** and parse a simple sentence using NLTK’s ChartParser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73939551-d7dd-40a6-963d-5ba05d7cdb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> Det N\n",
    "VP -> V NP\n",
    "Det -> 'the'\n",
    "N -> 'researcher' | 'dataset'\n",
    "V -> 'analyzed'\n",
    "\"\"\")\n",
    "\n",
    "parser = ChartParser(grammar)\n",
    "sentence = \"the researcher analyzed the dataset\".split()\n",
    "\n",
    "for tree in parser.parse(sentence):\n",
    "    tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7514f2-e94a-4e1c-8c47-3864b6505f45",
   "metadata": {},
   "source": [
    "## 7. RNN (LSTM) Text Classifier\n",
    "\n",
    "We classify AI vs non-AI grants using only the **abstract** text.\n",
    "\n",
    "Key idea:\n",
    "- RNNs maintain hidden state across tokens  \n",
    "- Useful for sequences and long-range dependencies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcded38-83c2-4a7f-90e0-d7cfffcce8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text…\n",
    "texts = grants[\"abstract\"].fillna(\"\").tolist()\n",
    "labels = grants[\"is_ai_ml\"].astype(int).values\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "X_seq = tokenizer.texts_to_sequences(texts)\n",
    "X_pad = preprocessing.sequence.pad_sequences(X_seq, maxlen=200)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pad, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "model_rnn = models.Sequential([\n",
    "    layers.Embedding(input_dim=5000, output_dim=64, input_length=200),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_rnn.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_rnn.fit(X_train, y_train, epochs=3, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "loss_rnn, acc_rnn = model_rnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"LSTM classification accuracy:\", acc_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aaa159-51e2-498e-901a-41dbb153dad6",
   "metadata": {},
   "source": [
    "## 8. Transformers & Attention\n",
    "\n",
    "Transformers use **self-attention** to process all words in parallel and capture global context.\n",
    "\n",
    "Here we:\n",
    "- Use a pretrained transformer (via TF Hub)\n",
    "- Generate sentence embeddings for grant abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b02b47-9106-4bf8-81bc-64860567bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "sentences = grants[\"abstract\"].dropna().head(5).tolist()\n",
    "embeddings = embed(sentences)\n",
    "\n",
    "print(\"Embedding shape:\", embeddings.shape)\n",
    "print(\"Similarity matrix:\\n\", np.inner(embeddings, embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7b48d-6e7b-4a5f-bbed-6df1c47a4bd5",
   "metadata": {},
   "source": [
    "## 9. Named Entity Recognition (NER)\n",
    "\n",
    "We extract scientific entities (institutions, diseases, compounds, locations) using spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d06234-6186-4dbf-b6c0-f609a88455a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = grants[\"abstract\"].dropna().iloc[0]\n",
    "doc = nlp(text)\n",
    "\n",
    "[(ent.text, ent.label_) for ent in doc.ents][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7974bc2-9030-4a2c-8a17-33e56d9b6dec",
   "metadata": {},
   "source": [
    "# Final Summary\n",
    "\n",
    "This notebook covered:\n",
    "\n",
    "### Core NLP Tasks\n",
    "- Tokenization, n-grams, Bag-of-Words\n",
    "- Text classification via Naive Bayes\n",
    "- Markov text generation\n",
    "\n",
    "### Word Representation\n",
    "- One-hot encoding, embeddings, Word2Vec Skip-Gram\n",
    "\n",
    "### Syntax & Semantics\n",
    "- Context-Free Grammar parsing with NLTK\n",
    "\n",
    "### Deep NLP\n",
    "- LSTM text classifier\n",
    "- Transformer embeddings & attention\n",
    "\n",
    "### Applications to Dimensions Data\n",
    "- Grant classification\n",
    "- Entity extraction\n",
    "- Similarity analysis\n",
    "- Text generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
