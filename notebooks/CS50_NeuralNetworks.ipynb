{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3460cf78-fd3e-455e-b217-caa9935011e0",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks with Dimensions Grant Data\n",
    "\n",
    "This notebook walks through key neural network concepts using **Dimensions-style grant data** as the running example.\n",
    "\n",
    "1. **Feed-forward ANNs (Dense networks)**  \n",
    "   - Inputs, weights, bias, activation functions (ReLU, sigmoid)  \n",
    "   - Gradient descent (mini-batch via Adam)  \n",
    "   - Binary classification: AI vs non-AI grants  \n",
    "\n",
    "2. **Logical functions with tiny networks**  \n",
    "   - Showing how a single neuron can learn AND / OR-like behavior  \n",
    "\n",
    "3. **Deep Neural Networks (DNNs) for regression**  \n",
    "   - Predicting 5-year citations from funding & topic scores  \n",
    "\n",
    "4. **Overfitting & Dropout**  \n",
    "   - Adding dropout layers to improve generalization  \n",
    "\n",
    "5. **1D Convolutional Neural Networks (CNN-style) on sequences**  \n",
    "   - Using yearly citation counts as a “time-series” input  \n",
    "\n",
    "6. **Recurrent Neural Networks (RNNs / LSTMs)**  \n",
    "   - Using citation sequences to classify grants  \n",
    "\n",
    "We assume a `grants` dataset exported from Dimensions into CSV or a DataFrame, with columns like:\n",
    "\n",
    "- `grant_id`\n",
    "- `topic_ai_score`, `topic_bioinfo_score`, `topic_data_repo_score`\n",
    "- `total_funding`\n",
    "- `citations_5yr`\n",
    "- `is_ai_ml` (0/1 label: is this an AI/ML-related grant?)\n",
    "- `citations_FY19`, `citations_FY20`, ..., `citations_FY25` (per-year citation counts)\n",
    "\n",
    "You can adapt column names to your actual pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e27b0-6528-4bbf-afe8-ef0bd61b2569",
   "metadata": {},
   "source": [
    "# Imports & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba08fd-fdde-4a42-b793-b2d8365f9d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# --- Load Dimensions-style grants data ---\n",
    "\n",
    "# Option 1: from CSV exported from Dimensions\n",
    "# grants = pd.read_csv(\"grants.csv\")\n",
    "\n",
    "# Option 2: if already in memory, just ensure it has the needed columns\n",
    "expected_cols = [\n",
    "    \"grant_id\",\n",
    "    \"topic_ai_score\",\n",
    "    \"topic_bioinfo_score\",\n",
    "    \"topic_data_repo_score\",\n",
    "    \"total_funding\",\n",
    "    \"citations_5yr\",\n",
    "    \"is_ai_ml\",\n",
    "    \"citations_FY19\", \"citations_FY20\", \"citations_FY21\",\n",
    "    \"citations_FY22\", \"citations_FY23\", \"citations_FY24\", \"citations_FY25\"\n",
    "]\n",
    "\n",
    "missing = [c for c in expected_cols if c not in globals().get(\"grants\", pd.DataFrame()).columns]\n",
    "if missing:\n",
    "    print(\"NOTE: grants DataFrame not found or missing columns.\")\n",
    "    print(\"Expected at least:\", expected_cols)\n",
    "    # You can uncomment the CSV line above and reload.\n",
    "else:\n",
    "    print(\"Grants data loaded with columns:\", grants.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a79d3-7f10-4318-97da-eeaffe3213b7",
   "metadata": {},
   "source": [
    "# Basic Feed-Forward ANN (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a776904-5552-456e-9a05-cd953be39692",
   "metadata": {},
   "source": [
    "## 1. Basic Feed-Forward ANN: Classifying AI vs Non-AI Grants\n",
    "\n",
    "**Goal:**  \n",
    "Use a simple artificial neural network to classify whether a grant is AI/ML-related (`is_ai_ml = 1`) based on numeric features such as topic scores and funding.\n",
    "\n",
    "**Key concepts:**\n",
    "\n",
    "- **Inputs:** topic scores, log funding  \n",
    "- **Weights & bias:** learned parameters in Dense layers  \n",
    "- **Activation functions:**\n",
    "  - ReLU in hidden layers (non-linear, `max(0, x)`)  \n",
    "  - Sigmoid in output layer (outputs in (0,1), interpretable as confidence)  \n",
    "- **Training:** mini-batch gradient descent via Adam optimizer  \n",
    "- **Loss:** binary cross-entropy for 0/1 classification  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f23c7e-36da-4acc-9620-2c78f45e4b61",
   "metadata": {},
   "source": [
    "# Feed-Forward ANN (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b6155-6476-48e1-9a76-1150aaacbb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare features and labels ---\n",
    "\n",
    "features = [\"topic_ai_score\", \"topic_bioinfo_score\", \"topic_data_repo_score\", \"total_funding\"]\n",
    "\n",
    "grants_nn = grants.copy()\n",
    "\n",
    "# Handle missing values\n",
    "grants_nn[features] = grants_nn[features].fillna(0.0)\n",
    "\n",
    "# Stabilize funding with log1p\n",
    "grants_nn[\"total_funding\"] = np.log1p(grants_nn[\"total_funding\"])\n",
    "\n",
    "X = grants_nn[features].values\n",
    "y = grants_nn[\"is_ai_ml\"].astype(int).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- Define the model: Input -> Dense(ReLU) -> Dense(ReLU) -> Dense(sigmoid) ---\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),   # input layer\n",
    "    layers.Dense(16, activation=\"relu\"),       # hidden layer with ReLU\n",
    "    layers.Dense(8, activation=\"relu\"),        # another hidden layer\n",
    "    layers.Dense(1, activation=\"sigmoid\")      # output layer with sigmoid\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,          # mini-batch gradient descent\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test accuracy (AI vs non-AI):\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc3624e-561f-4851-9f98-b24d8a4e5a29",
   "metadata": {},
   "source": [
    "# Logical Functions (AND / OR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5046f3-0f9e-462f-beb8-1e58d63ac64b",
   "metadata": {},
   "source": [
    "## 2. Logical Functions with a Tiny Network\n",
    "\n",
    "Before scaling to many inputs, show that a **single neuron** can represent simple logical functions like **AND** and **OR**.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Input 1: `has_ai` (1 if AI topic score > threshold, else 0)  \n",
    "- Input 2: `has_repo` (1 if data-repository topic score > threshold, else 0)  \n",
    "- Output: `1` if both are 1 (logical AND), else 0.  \n",
    "\n",
    "Train a one-layer network to learn this mapping directly from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893fbf11-e520-47fb-a900-d2ea74e0e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary flags from topic scores\n",
    "logic_df = grants.copy()\n",
    "logic_df[\"has_ai\"] = (logic_df[\"topic_ai_score\"] > 0.5).astype(int)\n",
    "logic_df[\"has_repo\"] = (logic_df[\"topic_data_repo_score\"] > 0.5).astype(int)\n",
    "\n",
    "# Define label for AND\n",
    "logic_df[\"label_and\"] = (logic_df[\"has_ai\"] & logic_df[\"has_repo\"]).astype(int)\n",
    "\n",
    "X_logic = logic_df[[\"has_ai\", \"has_repo\"]].values\n",
    "y_logic = logic_df[\"label_and\"].values\n",
    "\n",
    "model_and = keras.Sequential([\n",
    "    layers.Input(shape=(2,)),\n",
    "    layers.Dense(1, activation=\"sigmoid\")   # a single neuron with sigmoid\n",
    "])\n",
    "\n",
    "model_and.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Batch gradient descent: use all rows as a single batch for simplicity\n",
    "model_and.fit(\n",
    "    X_logic, y_logic,\n",
    "    epochs=500,\n",
    "    batch_size=len(X_logic),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "loss_and, acc_and = model_and.evaluate(X_logic, y_logic, verbose=0)\n",
    "print(\"Training accuracy (AND function):\", acc_and)\n",
    "print(\"Learned weights and bias:\", model_and.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fccf37-e02a-4ac2-a8f6-7e1a772aa889",
   "metadata": {},
   "source": [
    "# Deep Neural Network for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f35b85-05f4-4ceb-a12c-8e4ce03387f5",
   "metadata": {},
   "source": [
    "## 3. Deep Neural Network (DNN) for Regression\n",
    "\n",
    "**Goal:**  \n",
    "Predict **5-year citations** (`citations_5yr`) from grant features.\n",
    "\n",
    "**Concepts:**\n",
    "\n",
    "- **Continuous output** → use a single linear neuron (`activation=\"linear\"`) in the last layer.\n",
    "- **Loss function:** Mean Squared Error (L₂ loss) or MAE (L₁-like).\n",
    "- **Deep network:** multiple hidden layers with ReLU allow modeling non-linear relationships between funding, topic scores, and citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8922416-8d34-4801-9d5b-8819c907adf5",
   "metadata": {},
   "source": [
    "# DNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f367ad7-723a-4dbd-8059-bd0f7b39d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_reg = [\"topic_ai_score\", \"topic_bioinfo_score\", \"topic_data_repo_score\", \"total_funding\"]\n",
    "\n",
    "reg_df = grants.copy()\n",
    "reg_df[features_reg] = reg_df[features_reg].fillna(0.0)\n",
    "reg_df[\"total_funding\"] = np.log1p(reg_df[\"total_funding\"])\n",
    "reg_df[\"citations_5yr\"] = reg_df[\"citations_5yr\"].fillna(0.0)\n",
    "\n",
    "Xr = reg_df[features_reg].values\n",
    "yr = reg_df[\"citations_5yr\"].values\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "    Xr, yr, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "dnn = keras.Sequential([\n",
    "    layers.Input(shape=(Xr_train.shape[1],)),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"linear\")   # regression output\n",
    "])\n",
    "\n",
    "dnn.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "dnn.fit(\n",
    "    Xr_train, yr_train,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "test_mse, test_mae = dnn.evaluate(Xr_test, yr_test, verbose=0)\n",
    "print(\"Test MAE (citations regression):\", test_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1357e-a1f5-4a67-979d-8bf8dad6f39a",
   "metadata": {},
   "source": [
    "# Ovverfitting and Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b845eee1-acc4-4bda-9dfd-2140168f4a7d",
   "metadata": {},
   "source": [
    "## 4. Overfitting & Dropout\n",
    "\n",
    "**Overfitting:**  \n",
    "A model performs very well on training data but poorly on unseen data (test/validation).\n",
    "\n",
    "**Dropout:**  \n",
    "During training, randomly \"drops\" (disables) a fraction of units in a layer at each step.  \n",
    "This discourages the network from relying too heavily on any one path and usually **improves generalization**.\n",
    "\n",
    "Below: same classification model as before, but with **Dropout layer** added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a7bd0-7538-40da-b580-f156600f0ef9",
   "metadata": {},
   "source": [
    "# Classification with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792ccc0-ec67-4df6-82a9-808f9ce793fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_do = keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),              # randomly drop 50% of units during training\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_do.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history_do = model_do.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "loss_do, acc_do = model_do.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test accuracy with dropout:\", acc_do)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e8bd7-c57e-403b-a8a6-a9c1572d7076",
   "metadata": {},
   "source": [
    "# CNN on Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb76ac-67df-43d5-821d-9b430066efc7",
   "metadata": {},
   "source": [
    "## 5. 1D Convolutional Network on Citation Time Series\n",
    "\n",
    "Although CNNs are famous for **images**, the same ideas apply to **1D sequences**.\n",
    "\n",
    "We’ll use:\n",
    "\n",
    "- Input: yearly citations `citations_FY19`–`citations_FY25` (7 time steps).\n",
    "- Task: classify whether a grant is AI/ML (`is_ai_ml`).\n",
    "\n",
    "**Concepts:**\n",
    "\n",
    "- **Convolution (Conv1D):** sliding filters over the sequence to detect local patterns.\n",
    "- **Pooling (MaxPooling1D):** down-sampling to keep strongest responses and reduce size.\n",
    "- **Dense layers:** to map extracted features to a classification output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da93ad8-ec8b-4fe9-a5cd-02f69bddf561",
   "metadata": {},
   "source": [
    "# 1D CNN on Citation Sequenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce59f0f-a533-42de-a405-8cb15caf4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequence array: [citations_FY19 ... citations_FY25]\n",
    "year_cols = [f\"citations_FY{y}\" for y in range(19, 26)]\n",
    "\n",
    "seq_df = grants.copy()\n",
    "seq_df[year_cols] = seq_df[year_cols].fillna(0.0)\n",
    "seq_df[\"is_ai_ml\"] = seq_df[\"is_ai_ml\"].astype(int)\n",
    "\n",
    "X_seq = seq_df[year_cols].values  # (N, 7)\n",
    "y_seq = seq_df[\"is_ai_ml\"].values\n",
    "\n",
    "# Reshape for Conv1D: (samples, timesteps, channels)\n",
    "X_seq = X_seq[..., np.newaxis]  # add channel dim: (N, 7, 1)\n",
    "\n",
    "X_seq_train, X_seq_test, y_seq_train, y_seq_test = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, random_state=42, stratify=y_seq\n",
    ")\n",
    "\n",
    "cnn = keras.Sequential([\n",
    "    layers.Input(shape=(X_seq_train.shape[1], 1)),  # (7, 1)\n",
    "    layers.Conv1D(16, kernel_size=3, activation=\"relu\"),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Conv1D(32, kernel_size=3, activation=\"relu\"),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "cnn.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "cnn.fit(\n",
    "    X_seq_train, y_seq_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "loss_cnn, acc_cnn = cnn.evaluate(X_seq_test, y_seq_test, verbose=0)\n",
    "print(\"1D CNN accuracy (sequence-based AI classification):\", acc_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48721de7-2fa5-4bd4-883c-953a8f0495dc",
   "metadata": {},
   "source": [
    "# RNN / LSTM on Citation Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d7a3ed-2899-4c47-b262-0fc441a40d87",
   "metadata": {},
   "source": [
    "## 6. Recurrent Neural Network (LSTM) on Citation Sequences\n",
    "\n",
    "**Recurrent Neural Networks (RNNs)**, and especially **LSTMs**, are designed for **sequences** where past information matters.\n",
    "\n",
    "Here, we again use annual citations FY19–FY25 as a time series and try to classify whether a grant is AI/ML-related.\n",
    "\n",
    "**Concepts:**\n",
    "\n",
    "- **Recurrent connections:** an internal state carries information forward across time steps.\n",
    "- **Applications:** language, time-series forecasting, video, translation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e218bd7-128a-4d46-839d-9d82c183223c",
   "metadata": {},
   "source": [
    "# LSTM on Citation Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a765cb-11a3-4544-bc8a-8286270a2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq_train, X_seq_test, y_seq_train, y_seq_test = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, random_state=42, stratify=y_seq\n",
    ")\n",
    "\n",
    "rnn = keras.Sequential([\n",
    "    layers.Input(shape=(X_seq_train.shape[1], 1)),  # (7, 1)\n",
    "    layers.LSTM(32, return_sequences=False),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "rnn.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "rnn.fit(\n",
    "    X_seq_train, y_seq_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "loss_rnn, acc_rnn = rnn.evaluate(X_seq_test, y_seq_test, verbose=0)\n",
    "print(\"LSTM accuracy (sequence-based AI classification):\", acc_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9159003-e843-4e56-9262-3a19c84f5470",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "- Built **feed-forward ANNs** to classify AI vs non-AI grants and to predict citation counts.\n",
    "- Demonstrated how tiny networks can learn **logical functions** like AND.\n",
    "- Used **deep networks** (multiple hidden layers) to model non-linear relationships.\n",
    "- Illustrated **overfitting** and how **dropout** can improve generalization.\n",
    "- Applied **convolutional ideas** (Conv1D + pooling) to Dimensions **time-series** data.\n",
    "- Applied an **LSTM-based RNN** to yearly citations to classify grants.\n",
    "\n",
    "You can adapt:\n",
    "\n",
    "- **Inputs:** swap topic scores, abstract embeddings, funding, or PI-country indicators.  \n",
    "- **Outputs:** predict probability of being “high-impact,” funding decisions, or future citations.  \n",
    "- **Architectures:** deeper networks, different activations, or multi-task setups.\n",
    "\n",
    "Next steps (if you want to extend this):\n",
    "\n",
    "- Use **text embeddings** (e.g., from a transformer) of abstracts as ANN inputs.\n",
    "- Add **equity features** (country income level, region) and compare models with and without them.\n",
    "- Turn models into **screening tools** for highlighting potentially impactful or data-science-heavy grants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5036213-c19e-4ee4-bf88-e968de919bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
