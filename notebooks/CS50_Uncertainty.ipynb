{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d768db-8645-4f65-8fd5-f2572dd6f524",
   "metadata": {},
   "source": [
    "# Modeling Uncertainty & Probabilistic Reasoning with Dimensions Data\n",
    "\n",
    "This notebook shows how to model **uncertainty** in research trends and funding using probabilistic methods applied to Dimensions-style data.\n",
    "\n",
    "1. **Modeling Uncertainty in Research Trends**\n",
    "   - Estimate the probability that a topic’s publications increase next year.\n",
    "\n",
    "2. **Bayesian Inference for Funding Allocation**\n",
    "   - Estimate `P(Funding | Proposal Features)` and update as new data arrives.\n",
    "\n",
    "3. **Markov Models for Research Topic Evolution**\n",
    "   - Model topic transitions over time with a Markov chain.\n",
    "\n",
    "4. **Handling Missing Data with Probabilistic Models**\n",
    "   - Use model-based approaches (EM-style) to estimate missing citation counts.\n",
    "\n",
    "5. **Risk Assessment in Collaborative Research**\n",
    "   - Quantify risk of low impact for collaborative grants.\n",
    "\n",
    "Assumed data sources:\n",
    "\n",
    "- `pubs`: yearly publication counts by topic.\n",
    "- `grants`: grant-level data with features & outcomes.\n",
    "- `collabs`: subset or flag for collaborative projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6ad5a-3865-446f-9c52-f13b03406332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Example structures you can map to real Dimensions exports ---\n",
    "\n",
    "# 1) Yearly publication counts by topic\n",
    "# columns: topic, year, n_pubs\n",
    "# pubs = pd.read_csv(\"topic_publications_by_year.csv\")\n",
    "\n",
    "# 2) Grants / proposals dataset\n",
    "# columns (example):\n",
    "#   grant_id, year, primary_topic, funding_awarded (0/1),\n",
    "#   topic_ai_score, topic_data_repo_score, institution_tier,\n",
    "#   citations_5yr, is_collaborative, n_institutions\n",
    "# grants = pd.read_csv(\"grants.csv\")\n",
    "\n",
    "print(\"Pubs columns:\", pubs.columns.tolist())\n",
    "print(\"Grants columns:\", grants.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd207f-4db7-4d46-bbea-1c1b81bc352c",
   "metadata": {},
   "source": [
    "## 1. Modeling Uncertainty in Research Trends\n",
    "\n",
    "We want to quantify statements like:\n",
    "\n",
    "> “What is the probability that publications on topic **T** will increase next year?”\n",
    "\n",
    "Let:\n",
    "\n",
    "- `n_t` = number of publications on topic T in year t.\n",
    "- Define an “increase indicator”:\n",
    "  - \\( Y_t = 1 \\) if \\( n_t > n_{t-1} \\), else 0.\n",
    "\n",
    "We can estimate:\n",
    "\n",
    "\\[\n",
    "P(\\text{Increase next year for topic T} \\mid \\text{past trends})\n",
    "\\]\n",
    "\n",
    "A simple frequentist estimate:\n",
    "\n",
    "\\[\n",
    "\\hat{p} = \\frac{\\text{# years with increase}}{\\text{# year transitions}}\n",
    "\\]\n",
    "\n",
    "A simple Bayesian **Beta-Binomial** version:\n",
    "\n",
    "- Prior: \\( p \\sim \\mathrm{Beta}(\\alpha_0, \\beta_0) \\)\n",
    "- Data: \\( k \\) increases out of \\( N \\) transitions\n",
    "- Posterior: \\( \\mathrm{Beta}(\\alpha_0 + k, \\beta_0 + N - k) \\)\n",
    "- Posterior mean: \\( \\mathbb{E}[p] = \\frac{\\alpha_0 + k}{\\alpha_0 + \\beta_0 + N} \\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e1a87-288b-4e0d-9c9c-76e706959bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topic_increase_posterior(pubs, topic, alpha0=1.0, beta0=1.0):\n",
    "    \"\"\"\n",
    "    Given a 'pubs' DataFrame with columns (topic, year, n_pubs),\n",
    "    compute the posterior for the probability that a topic's\n",
    "    publications increase from year to year.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        pubs[pubs[\"topic\"] == topic]\n",
    "        .sort_values(\"year\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    if len(df) < 2:\n",
    "        raise ValueError(\"Need at least 2 years of data for this topic.\")\n",
    "\n",
    "    n = df[\"n_pubs\"].values\n",
    "    increases = (n[1:] > n[:-1]).astype(int)\n",
    "\n",
    "    k = increases.sum()             # # years with increase\n",
    "    N = len(increases)              # # transitions\n",
    "\n",
    "    alpha_post = alpha0 + k\n",
    "    beta_post = beta0 + (N - k)\n",
    "    posterior_mean = alpha_post / (alpha_post + beta_post)\n",
    "\n",
    "    return {\n",
    "        \"topic\": topic,\n",
    "        \"alpha_post\": alpha_post,\n",
    "        \"beta_post\": beta_post,\n",
    "        \"posterior_mean\": posterior_mean,\n",
    "        \"k_increases\": int(k),\n",
    "        \"N_transitions\": int(N),\n",
    "    }\n",
    "\n",
    "# Example for a specific topic:\n",
    "topic_example = pubs[\"topic\"].iloc[0]\n",
    "res = compute_topic_increase_posterior(pubs, topic_example, alpha0=1.0, beta0=1.0)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5645d676-e420-4f8c-a52a-6783ed65c7ce",
   "metadata": {},
   "source": [
    "## 2. Bayesian Inference for Funding Allocation\n",
    "\n",
    "We want \\( P(\\text{Funding} \\mid \\text{Proposal Features}) \\).\n",
    "\n",
    "**Bayes' theorem:**\n",
    "\n",
    "\\[\n",
    "P(F \\mid x) = \\frac{P(x \\mid F) P(F)}{P(x)}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( F \\) = event that a proposal is funded.\n",
    "- \\( x \\) = feature vector (e.g., topic score bins, institution tier).\n",
    "\n",
    "A practical approximation:\n",
    "\n",
    "1. Discretize features into a small number of categories.\n",
    "2. Estimate:\n",
    "   - \\( P(F) \\) (baseline funding rate).\n",
    "   - \\( P(x \\mid F) \\) and \\( P(x \\mid \\neg F) \\) as empirical frequencies.\n",
    "3. Use Bayes’ rule to compute \\( P(F \\mid x) \\) for new proposals.\n",
    "\n",
    "We’ll show a simple **Naive Bayes-style** implementation over binned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d75c247-fc9a-42f6-be02-b33991df8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple categorical features from grants:\n",
    "gr = grants.copy()\n",
    "\n",
    "# Example binning:\n",
    "gr[\"ai_bin\"] = pd.cut(gr[\"topic_ai_score\"].fillna(0.0),\n",
    "                      bins=[-np.inf, 0.2, 0.5, 1.0],\n",
    "                      labels=[\"low\", \"medium\", \"high\"])\n",
    "\n",
    "gr[\"inst_bin\"] = gr[\"institution_tier\"].fillna(\"unknown\")\n",
    "\n",
    "# We'll use a combined feature x = (ai_bin, inst_bin)\n",
    "gr[\"feature_combo\"] = gr[\"ai_bin\"].astype(str) + \"|\" + gr[\"inst_bin\"].astype(str)\n",
    "\n",
    "# Funding indicator F\n",
    "gr[\"F\"] = gr[\"funding_awarded\"].astype(int)\n",
    "\n",
    "# Prior P(F)\n",
    "p_F = gr[\"F\"].mean()\n",
    "p_notF = 1 - p_F\n",
    "\n",
    "# Likelihood P(x | F) and P(x | not F) as empirical frequencies\n",
    "counts_F = gr[gr[\"F\"] == 1][\"feature_combo\"].value_counts()\n",
    "counts_notF = gr[gr[\"F\"] == 0][\"feature_combo\"].value_counts()\n",
    "all_x = set(counts_F.index).union(set(counts_notF.index))\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1.0\n",
    "\n",
    "likelihood_F = {}\n",
    "likelihood_notF = {}\n",
    "\n",
    "for x_val in all_x:\n",
    "    likelihood_F[x_val] = (counts_F.get(x_val, 0) + alpha) / (counts_F.sum() + alpha * len(all_x))\n",
    "    likelihood_notF[x_val] = (counts_notF.get(x_val, 0) + alpha) / (counts_notF.sum() + alpha * len(all_x))\n",
    "\n",
    "def bayes_funding_prob(ai_bin, inst_bin):\n",
    "    \"\"\"\n",
    "    Compute P(Funding | ai_bin, inst_bin) using the Naive Bayes-style model.\n",
    "    \"\"\"\n",
    "    x_val = f\"{ai_bin}|{inst_bin}\"\n",
    "    # if unseen combo, fall back to uniform likelihood\n",
    "    if x_val not in all_x:\n",
    "        like_F = 1 / len(all_x)\n",
    "        like_notF = 1 / len(all_x)\n",
    "    else:\n",
    "        like_F = likelihood_F[x_val]\n",
    "        like_notF = likelihood_notF[x_val]\n",
    "\n",
    "    # Bayes\n",
    "    numerator = like_F * p_F\n",
    "    denominator = like_F * p_F + like_notF * p_notF\n",
    "    return numerator / denominator if denominator > 0 else p_F\n",
    "\n",
    "# Example: medium AI score, Tier1 institution\n",
    "bayes_funding_prob(\"medium\", \"Tier1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4721252-0930-4fe0-8e09-7895c1bad754",
   "metadata": {},
   "source": [
    "## 3. Markov Models for Research Topic Evolution\n",
    "\n",
    "We want to model how topics **transition** over time.\n",
    "\n",
    "Example:\n",
    "\n",
    "- States: high-level topics (e.g., AI/ML, DRKB, DMC, CB/SM, etc.).\n",
    "- Transition: topic at year t → topic at year t+1.\n",
    "\n",
    "We build a **Markov chain**:\n",
    "\n",
    "- Transition matrix \\( P_{ij} = P(\\text{Topic}_{t+1} = j \\mid \\text{Topic}_t = i) \\).\n",
    "- Use it to:\n",
    "  - Analyze how research focus shifts.\n",
    "  - Simulate possible future trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7df5a46-cbad-40ee-8ad6-f5ad5f531286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume grants has columns: grant_id, year, primary_topic\n",
    "\n",
    "# Step 1: get unique topics and index them\n",
    "topics = sorted(grants[\"primary_topic\"].dropna().unique())\n",
    "topic_to_idx = {t: i for i, t in enumerate(topics)}\n",
    "n_topics = len(topics)\n",
    "\n",
    "# Step 2: construct transitions from year to year\n",
    "# For simplicity, aggregate at (topic, year) and look at dominant transitions\n",
    "topic_year = (\n",
    "    grants.groupby([\"year\", \"primary_topic\"])[\"grant_id\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"n_grants\")\n",
    ")\n",
    "\n",
    "topic_year = topic_year.sort_values([\"primary_topic\", \"year\"])\n",
    "\n",
    "# Build counts of transitions topic_t -> topic_{t+1}\n",
    "transition_counts = np.zeros((n_topics, n_topics), dtype=float)\n",
    "\n",
    "# Here we assume each topic \"stays\" in itself across years, but we can\n",
    "# measure shifts using dominant topic changes if you track primary_topic per grant.\n",
    "# For illustration, we just model persistence + noise from aggregated data:\n",
    "\n",
    "for t_idx, topic in enumerate(topics):\n",
    "    topic_data = topic_year[topic_year[\"primary_topic\"] == topic].sort_values(\"year\")\n",
    "    years = topic_data[\"year\"].values\n",
    "    # treat each consecutive year pair as a self-transition\n",
    "    for y1, y2 in zip(years[:-1], years[1:]):\n",
    "        transition_counts[t_idx, t_idx] += 1\n",
    "\n",
    "# Add a little smoothing noise (e.g., topic diffusion)\n",
    "epsilon = 0.01\n",
    "transition_counts += epsilon\n",
    "\n",
    "# Step 3: normalize rows to sum to 1 to get transition matrix\n",
    "row_sums = transition_counts.sum(axis=1, keepdims=True)\n",
    "P = transition_counts / row_sums\n",
    "\n",
    "pd.DataFrame(P, index=topics, columns=topics).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6bbdc-5d15-4410-b03f-d55544a89c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_topic_chain(P, topics, start_topic, n_steps=5):\n",
    "    topic_to_idx = {t: i for i, t in enumerate(topics)}\n",
    "    idx_to_topic = {i: t for i, t in enumerate(topics)}\n",
    "\n",
    "    current_idx = topic_to_idx[start_topic]\n",
    "    trajectory = [start_topic]\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        probs = P[current_idx]\n",
    "        next_idx = np.random.choice(len(topics), p=probs)\n",
    "        trajectory.append(idx_to_topic[next_idx])\n",
    "        current_idx = next_idx\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "simulate_topic_chain(P, topics, start_topic=topics[0], n_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656afc8-c8bc-4cb3-8441-c3f0e41df7d6",
   "metadata": {},
   "source": [
    "## 4. Handling Missing Citation Data with Probabilistic Models\n",
    "\n",
    "Citation data often has missing values (e.g., incomplete time windows, delayed indexing).\n",
    "\n",
    "We can:\n",
    "\n",
    "- Assume citations follow a mixture of \"low-impact\" and \"high-impact\" distributions.\n",
    "- Use an **EM-style** approach:\n",
    "  - **E-step:** estimate probability each grant is low/high impact.\n",
    "  - **M-step:** update parameters (means/variances).\n",
    "- Use the learned parameters to **impute missing citations**.\n",
    "\n",
    "For simplicity, we’ll:\n",
    "\n",
    "- Cluster citation counts into 2 Gaussian components (low/high).\n",
    "- Use cluster means for imputing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec42e57-ee49-4f40-997e-9a42ab572f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Take a copy and isolate the citations column\n",
    "cit_df = grants[[\"grant_id\", \"citations_5yr\"]].copy()\n",
    "\n",
    "# Mask missing\n",
    "mask_missing = cit_df[\"citations_5yr\"].isna()\n",
    "observed = cit_df[~mask_missing][\"citations_5yr\"].values.reshape(-1, 1)\n",
    "\n",
    "# Fit a 2-component Gaussian mixture to observed citations\n",
    "gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "gmm.fit(observed)\n",
    "\n",
    "# For missing ones, we can impute using the overall expected value under the mixture\n",
    "overall_mean = (gmm.weights_ * gmm.means_.flatten()).sum()\n",
    "print(\"Overall mixture mean:\", overall_mean)\n",
    "\n",
    "# Simple imputation: fill missing with mixture mean (or choose component-specific mean if you have other predictors)\n",
    "cit_df.loc[mask_missing, \"citations_5yr_imputed\"] = overall_mean\n",
    "cit_df.loc[~mask_missing, \"citations_5yr_imputed\"] = cit_df.loc[~mask_missing, \"citations_5yr\"]\n",
    "\n",
    "cit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d41e3-c921-4fe1-839e-79be46555d34",
   "metadata": {},
   "source": [
    "## 5. Risk Assessment in Collaborative Research\n",
    "\n",
    "Collaborative projects can have varying impact, and we’d like to quantify **risk**:\n",
    "\n",
    "> “What is the probability that a collaborative project has **low impact**?”\n",
    "\n",
    "Example:\n",
    "\n",
    "1. Define impact categories based on citations:\n",
    "   - Low: citations_5yr < 10\n",
    "   - Medium: 10–50\n",
    "   - High: > 50\n",
    "\n",
    "2. Filter to **collaborative** projects (`is_collaborative == 1`).\n",
    "\n",
    "3. Estimate:\n",
    "   - \\( P(\\text{Low impact} \\mid \\text{collab type, topic, etc.}) \\)\n",
    "   - Use historical data + Bayesian smoothing (Dirichlet prior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f4cde-61a9-4934-9370-f382e63b9572",
   "metadata": {},
   "outputs": [],
   "source": [
    "collabs = grants[grants[\"is_collaborative\"] == 1].copy()\n",
    "\n",
    "# Define impact categories\n",
    "def impact_category(cites):\n",
    "    if cites < 10:\n",
    "        return \"low\"\n",
    "    elif cites <= 50:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "collabs[\"impact_cat\"] = collabs[\"citations_5yr\"].fillna(0.0).apply(impact_category)\n",
    "\n",
    "# Example stratification: by primary_topic and n_institutions bin\n",
    "collabs[\"inst_bin\"] = pd.cut(collabs[\"n_institutions\"].fillna(1),\n",
    "                             bins=[0, 2, 5, np.inf],\n",
    "                             labels=[\"small\", \"medium\", \"large\"])\n",
    "\n",
    "collabs[\"group_key\"] = collabs[\"primary_topic\"].astype(str) + \"|\" + collabs[\"inst_bin\"].astype(str)\n",
    "\n",
    "# Dirichlet prior alpha for each category\n",
    "alpha_prior = {\"low\": 1.0, \"medium\": 1.0, \"high\": 1.0}\n",
    "impact_levels = [\"low\", \"medium\", \"high\"]\n",
    "\n",
    "def impact_risk_for_group(df_group):\n",
    "    counts = df_group[\"impact_cat\"].value_counts().to_dict()\n",
    "    total = sum(counts.get(k, 0) for k in impact_levels)\n",
    "\n",
    "    alpha_post = {\n",
    "        lvl: alpha_prior[lvl] + counts.get(lvl, 0)\n",
    "        for lvl in impact_levels\n",
    "    }\n",
    "    alpha_sum = sum(alpha_post.values())\n",
    "\n",
    "    # Posterior probabilities\n",
    "    post_probs = {lvl: alpha_post[lvl] / alpha_sum for lvl in impact_levels}\n",
    "    return post_probs\n",
    "\n",
    "# Compute risk per group\n",
    "group_risk = (\n",
    "    collabs.groupby(\"group_key\")\n",
    "    .apply(impact_risk_for_group)\n",
    ")\n",
    "\n",
    "# Example: inspect one group’s risk (posterior P(low/medium/high))\n",
    "group_risk.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bc7b3d-0b42-42ab-811d-bd5c658150bc",
   "metadata": {},
   "source": [
    "1. **Modeling Uncertainty in Trends**\n",
    "   - Bayesian Beta-Binomial model for probability of publication increases per topic.\n",
    "\n",
    "2. **Bayesian Inference for Funding**\n",
    "   - Simple Naive Bayes-style model for \\( P(\\text{Funding} \\mid \\text{Features}) \\).\n",
    "\n",
    "3. **Markov Models for Topic Evolution**\n",
    "   - Transition matrix and simulations of topic trajectories.\n",
    "\n",
    "4. **Missing Data Handling**\n",
    "   - EM-like imputation with a Gaussian mixture model for citations.\n",
    "\n",
    "5. **Risk Assessment in Collaboration**\n",
    "   - Dirichlet-smoothed probabilities of low/medium/high impact for collaborative groups.\n",
    "\n",
    "These techniques help **quantify uncertainty**, build **probabilistic forecasts**, and support **decision-making** in research funding and portfolio analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682860e-cd91-493d-b8c4-ad30a56a269e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
